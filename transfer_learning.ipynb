{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ekurenty/dotfiles/blob/master/transfer_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqNDta-xZ2u4"
      },
      "source": [
        "# Transfer Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xp81GSFN7uHY"
      },
      "source": [
        "* ü§Ø Tech companies and university labs have more computational resources than we do\n",
        "* üòé Let them train their super complex models on millions of images, and then re-use their kernels for our own CNNs!\n",
        "\n",
        "üéØ **<u>Goal:</u>**\n",
        "* ‚òÑÔ∏è Use a **Pretrained Neural Network** $ \\Leftrightarrow $ **Transfer learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LR9-yIauZ2vA"
      },
      "source": [
        "## Google Colab Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ieSm6iw9HizE"
      },
      "source": [
        "Repeat the same process from the last challenge to upload your challenge folder and open your notebook:\n",
        "\n",
        "1. access your [Google Drive](https://drive.google.com/)\n",
        "2. go into the Colab Notebooks folder\n",
        "3. drag and drop this challenge's folder into it\n",
        "4. right-click the notebook file and select `Open with` $\\rightarrow$ `Google Colaboratory`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YfeVhhBZ2vC"
      },
      "source": [
        "Don't forget to enable GPU acceleration!\n",
        "\n",
        "`Runtime` $\\rightarrow$ `Change runtime type` $\\rightarrow$ `Hardware accelerator` $\\rightarrow$ `GPU`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoRPmZtQZ2vE"
      },
      "source": [
        "When this is done, run the cells below and get to work!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_q8uOTzh5vwJ",
        "outputId": "a8bbffb9-a5f6-47bf-d2b0-408523645079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount GDrive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "7HCO04lsIrcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 180
        },
        "outputId": "4ea6d116-76d8-4be1-85d5-632644eddfdc"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/data-transfer-learning'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-45d03014822d>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# os.chdir allows you to change directories, like cd in the Terminal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Colab Notebooks/data-transfer-learning'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Colab Notebooks/data-transfer-learning'"
          ]
        }
      ],
      "source": [
        "# Put Colab in the context of this challenge\n",
        "import os\n",
        "\n",
        "# os.chdir allows you to change directories, like cd in the Terminal\n",
        "os.chdir('/content/drive/MyDrive/Colab Notebooks/data-transfer-learning')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0MdAwhGJdSR"
      },
      "source": [
        "You are now good to go, proceed with the challenge! Don't forget to copy everything back to your PC to upload to Kitt üöÄ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zsdd1Jv7Z2vJ"
      },
      "source": [
        "## (1) What is a Pre-Trained Neural Network?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shgjzVW4Z2vJ"
      },
      "source": [
        "* Convolutions are mathematical operations designed to detect specific patterns in input images and use them to classify the images.\n",
        "* One could imagine that these patterns are not 100% specific to one task but to the input images.\n",
        "\n",
        "üöÄ **Why not re-use these kernels - whose weights have already been optimized - somewhere else?**\n",
        "- The expectation is that the trained kernels could also help us perform another classification task.\n",
        "- We are trying to ***transfer*** the knowledge of a trained CNN to a new classification task.\n",
        "\n",
        "\n",
        "üí™ Transfer Learning has two main advantages:\n",
        "- It takes less time to train a pre-trained model since we are not going to update all the weights but only some of them\n",
        "- You benefit from state-of-the-art architectures that have been trained on complex images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sOkQIQj4Z2vJ"
      },
      "source": [
        "## (2) Introduction to  VGG16\n",
        "\n",
        "üìö ***Reading Section, no code***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAkNOgtqF7S_"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "In this exercise, we will use the <a href=\"https://neurohive.io/en/popular-networks/vgg16/\">**`VGG-16 Neural Network`**</a>.\n",
        "\n",
        "> VGG16 is a convolutional neural network model proposed by K. Simonyan and A. Zisserman from the University of Oxford in the paper ‚ÄúVery Deep Convolutional Networks for Large-Scale Image Recognition‚Äù. The model achieves 92.7% top-5 test accuracy in ImageNet, which is a dataset of over 14 million images belonging to 1000 classes. It was one of the famous model submitted to ILSVRC-2014. It makes the improvement over AlexNet by replacing large kernel-sized filters (11 and 5 in the first and second convolutional layer, respectively) with multiple 3√ó3 kernel-sized filters one after another. VGG16 was trained for weeks and was using NVIDIA Titan Black GPU‚Äôs.\n",
        "\n",
        "VGG16 is a well-known architecture that has been trained on the <a href=\"https://www.image-net.org/\">**`ImageNet dataset`**</a> which is a very large database of images which belong to different categories.\n",
        "\n",
        "üëâ This architecture already learned which kernels are the best for extracting features from the images found in the `ImageNet dataset`.\n",
        "\n",
        "üëâ As you can see in the illustration, the VGG16 involves millions of parameters you don't want to retrain yourself.\n",
        "\n",
        "\n",
        "<center><img src=\"https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png\" width=400></center>\n",
        "\n",
        "‚ùì How does it work in practice ‚ùì\n",
        "\n",
        "* The first layers are not specialized for the particular task the VGG16 CNN was trained on\n",
        "* Only the last dense layer is a \"classification layers\" that can be preceded with a couple of dense layers...  Therefore, we will:\n",
        "    1. Load the existing VGG16 network\n",
        "    2. Remove the last fully connected layers\n",
        "    3. Replace them with some new fully-connected layers (whose weights are randomly set)\n",
        "    4. Train these last layers on a specific classification task.\n",
        "\n",
        "üòÉ Your role is to train only the last layers for your particular problem.\n",
        "\n",
        "ü§ì We will use <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg16/VGG16\">**`tensorflow.keras.applications.VGG16`**</a>\n",
        "\n",
        "<hr>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-S858KRF7TA"
      },
      "source": [
        "## (3) Data loading & Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzLvo4N5Z2vL"
      },
      "source": [
        "You have two options to load the data into Google Colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6vPgeK2Z2vL"
      },
      "source": [
        "### (Option 1) Loading the data directly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6FxEU-HZ2vL"
      },
      "source": [
        "* You can first get the data onto google Colab thanks to:\n",
        "\n",
        "`!wget https://wagon-public-datasets.s3.amazonaws.com/flowers-dataset.zip`,\n",
        "\n",
        "* and then run\n",
        "\n",
        "`!unzip flowers-dataset.zip`\n",
        "\n",
        "*This is a very easy option to load the data into your working directory.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ID413NdhZ2vL"
      },
      "source": [
        "### (Option 2) Adding the data to Google Drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsnMYzYgZ2vM"
      },
      "source": [
        "* You can first download the data from `https://wagon-public-datasets.s3.amazonaws.com/flowers-dataset.zip`.\n",
        "* Then you have to add it to your Google Drive in a folder called `Deep_learning_data` (for instance)\n",
        "* And run the following code in the notebook:\n",
        "\n",
        "```python\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "```\n",
        "\n",
        "* The previous code will ask you to go to a given webpage where you can copy a temporary key\n",
        "* Paste it in the cell that will appear in your Colab Notebook\n",
        "* You can now load the data on your Google Colab Notebooks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yrUUBluZ2vM"
      },
      "source": [
        "### Option 1 or Option 2 ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdS5hErJF7TB"
      },
      "source": [
        "* Why choosing option 2 over the option 1?\n",
        "    * ‚úÖ The combo Colab + Drive can be interesting if you work within a project team, and need to update the data from time to time.\n",
        "    * ‚úÖ By doing this, you can share the same data folder with your teammates, and be sure that everyone has the same dataset at any time, even though someone changes it.\n",
        "    * ‚ùå Google Colab has now access to your Google Folder..., which you may or may not be in favor of, depending on your sensibilities...\n",
        "\n",
        "---\n",
        "\n",
        "‚ùì **Question: Loading your dataset** ‚ùì\n",
        "    \n",
        "Use one of the above methods to load your data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "hKw_1TjOF7TC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39bcaca6-494c-4161-e63d-d155937f66c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-21 08:43:03--  https://wagon-public-datasets.s3.amazonaws.com/flowers-dataset.zip\n",
            "Resolving wagon-public-datasets.s3.amazonaws.com (wagon-public-datasets.s3.amazonaws.com)... 52.218.46.25, 52.92.20.25, 52.218.37.122, ...\n",
            "Connecting to wagon-public-datasets.s3.amazonaws.com (wagon-public-datasets.s3.amazonaws.com)|52.218.46.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 104983809 (100M) [application/zip]\n",
            "Saving to: ‚Äòflowers-dataset.zip.1‚Äô\n",
            "\n",
            "flowers-dataset.zip 100%[===================>] 100.12M  16.4MB/s    in 7.3s    \n",
            "\n",
            "2024-02-21 08:43:11 (13.7 MB/s) - ‚Äòflowers-dataset.zip.1‚Äô saved [104983809/104983809]\n",
            "\n",
            "Archive:  flowers-dataset.zip\n",
            "replace flowers/daisy/2607132536_d95198e619_n.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace flowers/daisy/6480809771_b1e14c5cc2_m.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace flowers/daisy/9350942387_5b1d043c26_n.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: [n]\n",
            "error:  invalid response [[n]]\n",
            "replace flowers/daisy/9350942387_5b1d043c26_n.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
            "replace flowers/daisy/4065883015_4bb6010cb7_n.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: flowers/daisy/4065883015_4bb6010cb7_n.jpg  \n",
            "replace flowers/daisy/7924174040_444d5bbb8a.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: no\n",
            "replace flowers/daisy/8740807508_0587f5b7b7.jpg? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ],
      "source": [
        "option_1 = True # Choose here\n",
        "\n",
        "if option_1:\n",
        "    !wget https://wagon-public-datasets.s3.amazonaws.com/flowers-dataset.zip\n",
        "    !unzip flowers-dataset.zip\n",
        "else:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dJbUgwnzKNn",
        "outputId": "4ea0b7de-615f-43cd-c29c-4430bcabd0b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PeFQebcUzKNn",
        "outputId": "08c24be5-9daf-4247-c5f7-5c6bf72eed60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  flowers\tflowers-dataset.zip  flowers-dataset.zip.1  sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onPwIzQzF7TE"
      },
      "source": [
        "‚ùì **Question:Train/Val/Test split** ‚ùì\n",
        "\n",
        "Use the following method to create\n",
        "`X_train, y_train, X_val, y_val, X_test, y_test, num_classes` depending on the `loading_method` you have used"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "JksrDM0VzKNo"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "def load_flowers_data(loading_method):\n",
        "    if loading_method == 'colab':\n",
        "        data_path = '/content/flowers'\n",
        "    elif loading_method == 'direct':\n",
        "        data_path = 'flowers/'\n",
        "    classes = {'daisy':0, 'dandelion':1, 'rose':2}\n",
        "    imgs = []\n",
        "    labels = []\n",
        "    for (cl, i) in classes.items():\n",
        "        images_path = [elt for elt in os.listdir(os.path.join(data_path, cl)) if elt.find('.jpg')>0]\n",
        "        for img in tqdm(images_path[:300]):\n",
        "            path = os.path.join(data_path, cl, img)\n",
        "            if os.path.exists(path):\n",
        "                image = Image.open(path)\n",
        "                image = image.resize((256, 256))\n",
        "                imgs.append(np.array(image))\n",
        "                labels.append(i)\n",
        "\n",
        "    X = np.array(imgs)\n",
        "    num_classes = len(set(labels))\n",
        "    y = to_categorical(labels, num_classes)\n",
        "\n",
        "    # Finally we shuffle:\n",
        "    p = np.random.permutation(len(X))\n",
        "    X, y = X[p], y[p]\n",
        "\n",
        "    first_split = int(len(imgs) /6.)\n",
        "    second_split = first_split + int(len(imgs) * 0.2)\n",
        "    X_test, X_val, X_train = X[:first_split], X[first_split:second_split], X[second_split:]\n",
        "    y_test, y_val, y_train = y[:first_split], y[first_split:second_split], y[second_split:]\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test, num_classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QasZ2ZCrzKNo",
        "outputId": "c07d39dc-5a00-4cb1-d7cc-8dcb0aefa5bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [00:00<00:00, 314.25it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300/300 [00:00<00:00, 309.99it/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 299/299 [00:00<00:00, 302.20it/s]\n"
          ]
        }
      ],
      "source": [
        "# CALL load_flowers_data WITH YOUR PREFERRED METHOD HERE\n",
        "X_train, y_train, X_val, y_val, X_test, y_test, num_classes = load_flowers_data('direct')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, y_train.shape, X_val.shape, y_val.shape, X_test.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2Tz_nnz2tFX",
        "outputId": "4d18af4c-d89e-45e2-de72-15b835789526"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((571, 256, 256, 3),\n",
              " (571, 3),\n",
              " (179, 256, 256, 3),\n",
              " (179, 3),\n",
              " (149, 256, 256, 3),\n",
              " (149, 3))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ipehi0TJ29A8",
        "outputId": "41e84b12-3a27-4dfe-e4ee-2f90b1ebfd55"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 0., 0.],\n",
              "       [0., 1., 0.],\n",
              "       [1., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 1.],\n",
              "       [0., 0., 1.],\n",
              "       [0., 1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pm2IsMmGF7TH"
      },
      "source": [
        "‚ùì **Question: Exploring the images** ‚ùì\n",
        "\n",
        "Check the images' shapes and plot a few of them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdMfP6dyF7TJ"
      },
      "source": [
        "## (4) A CNN architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njYcWjw1F7TJ"
      },
      "source": [
        "First, let's build our own CNN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wneAeLYdF7TK"
      },
      "source": [
        "‚ùì **Questions** ‚ùì\n",
        "\n",
        "1. <u>CNN Architecture and compiler:</u> Create a CNN with your own architecture and a function `load_own_model` that will be able to generate it. Some advice:\n",
        "    - Incorporate the Rescaling Layer in your Sequential architecture\n",
        "    - Add three Conv2D/MaxPooling2D combinations with an increasing number of channels and a decreasing size of kernels for example (be creative, that is not a rule of thumb, mastering CNN is an art)\n",
        "    - Don't forget the Flatten layer and some hidden layers\n",
        "    - Finish with the predictive layer\n",
        "    - Compile your CNN model accordingly\n",
        "  \n",
        "  \n",
        "2. <u>Training and comparison</u>:\n",
        "    - Train your CNN\n",
        "    - Compare its performance to a baseline accuracy\n",
        "\n",
        "<details>\n",
        "    <summary><i>Recommended architecture:</i></summary>\n",
        "\n",
        "```python\n",
        "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# Notice this cool new layer that \"pipe\" your rescaling within the architecture\n",
        "model.add(Rescaling(1./255, input_shape=(256,256,3)))\n",
        "\n",
        "# Lets add 3 convolution layers, with relatively large kernel size as our pictures are quite big too\n",
        "model.add(layers.Conv2D(16, kernel_size=10, activation='relu'))\n",
        "model.add(layers.MaxPooling2D(3))\n",
        "\n",
        "model.add(layers.Conv2D(32, kernel_size=8, activation=\"relu\"))\n",
        "model.add(layers.MaxPooling2D(3))\n",
        "\n",
        "model.add(layers.Conv2D(32, kernel_size=6, activation=\"relu\"))\n",
        "model.add(layers.MaxPooling2D(3))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(100, activation='relu'))\n",
        "model.add(layers.Dense(3, activation='softmax'))\n",
        "```\n",
        "\n",
        "        \n",
        "</details>        "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import Rescaling\n",
        "from tensorflow.keras import Sequential, layers, optimizers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "def load_model():\n",
        "  model = Sequential()\n",
        "\n",
        "  # Notice this cool new layer that \"pipe\" your rescaling within the architecture\n",
        "  model.add(Rescaling(1./255, input_shape=(256,256,3)))\n",
        "\n",
        "  # Lets add 3 convolution layers, with relatively large kernel size as our pictures are quite big too\n",
        "  model.add(layers.Conv2D(16, kernel_size=10, activation='relu'))\n",
        "  model.add(layers.MaxPooling2D(3))\n",
        "\n",
        "  model.add(layers.Conv2D(32, kernel_size=8, activation=\"relu\"))\n",
        "  model.add(layers.MaxPooling2D(3))\n",
        "\n",
        "  model.add(layers.Conv2D(32, kernel_size=6, activation=\"relu\"))\n",
        "  model.add(layers.MaxPooling2D(3))\n",
        "\n",
        "  model.add(layers.Flatten())\n",
        "  model.add(layers.Dense(100, activation='relu'))\n",
        "  model.add(layers.Dense(3, activation='softmax'))\n",
        "\n",
        "  model.compile(loss= 'categorical_crossentropy', optimizer = 'adam', metrics = 'accuracy')\n",
        "\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "yb74G5N_3qhu"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_model = load_model()"
      ],
      "metadata": {
        "id": "FhXIkIGH4PKz"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3sUDTEQ4_AT",
        "outputId": "c5fd28fb-2687-4867-ef4f-a22b789cd153"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " rescaling_2 (Rescaling)     (None, 256, 256, 3)       0         \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 247, 247, 16)      4816      \n",
            "                                                                 \n",
            " max_pooling2d_6 (MaxPoolin  (None, 82, 82, 16)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 75, 75, 32)        32800     \n",
            "                                                                 \n",
            " max_pooling2d_7 (MaxPoolin  (None, 25, 25, 32)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " conv2d_8 (Conv2D)           (None, 20, 20, 32)        36896     \n",
            "                                                                 \n",
            " max_pooling2d_8 (MaxPoolin  (None, 6, 6, 32)          0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 1152)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 100)               115300    \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 3)                 303       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 190115 (742.64 KB)\n",
            "Trainable params: 190115 (742.64 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "es = EarlyStopping(patience = 5, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "C4_Gyma85IC4"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = my_model.fit(X_train, y_train, validation_data = (X_val, y_val), epochs = 100, batch_size=16, verbose = 1, callbacks = [es])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ok2IMLuA5X8N",
        "outputId": "c358ccb0-4c07-497a-cacc-f91264bc5857"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "36/36 [==============================] - 8s 57ms/step - loss: 0.9632 - accuracy: 0.4939 - val_loss: 0.8344 - val_accuracy: 0.5419\n",
            "Epoch 2/100\n",
            "36/36 [==============================] - 1s 25ms/step - loss: 0.8312 - accuracy: 0.5604 - val_loss: 0.8022 - val_accuracy: 0.5698\n",
            "Epoch 3/100\n",
            "36/36 [==============================] - 1s 25ms/step - loss: 0.8449 - accuracy: 0.5692 - val_loss: 0.8462 - val_accuracy: 0.5754\n",
            "Epoch 4/100\n",
            "36/36 [==============================] - 1s 26ms/step - loss: 0.8432 - accuracy: 0.5587 - val_loss: 0.8891 - val_accuracy: 0.5196\n",
            "Epoch 5/100\n",
            "36/36 [==============================] - 1s 23ms/step - loss: 0.7975 - accuracy: 0.6305 - val_loss: 0.8302 - val_accuracy: 0.6201\n",
            "Epoch 6/100\n",
            "36/36 [==============================] - 1s 22ms/step - loss: 0.7593 - accuracy: 0.6620 - val_loss: 0.8114 - val_accuracy: 0.6536\n",
            "Epoch 7/100\n",
            "36/36 [==============================] - 1s 24ms/step - loss: 0.6952 - accuracy: 0.7110 - val_loss: 0.8027 - val_accuracy: 0.6872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res = my_model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oLYBolyH6UQG",
        "outputId": "2eaf25c8-b1f9-4a58-f349-cb4f58d39355"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 1s 121ms/step - loss: 0.8953 - accuracy: 0.4832\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NesYIGrWzKNo",
        "outputId": "81aecc7e-e201-421f-ffa7-2e4903450107"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_accuracy = 48.0 %\n"
          ]
        }
      ],
      "source": [
        "test_accuracy = res[-1]\n",
        "print(f\"test_accuracy = {round(test_accuracy,2)*100} %\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DC2SFwUZ2vR"
      },
      "source": [
        "ü•° <b><u>Takeaways from building your own CNN</u></b>:\n",
        "* On an \"easy dataset\" like the MNIST, it is now easy to reach a decent accuracy. But for a more complicated problem like classifying flowers, it already becomes more challenging. Take a few minutes to play with the following link before moving on to Transfer Learning\n",
        "    * [PoloClub/CNN-Explainer](https://poloclub.github.io/cnn-explainer/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sx8V9sny7cLP"
      },
      "source": [
        "## (5) Using a pre-trained CNN = Transfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxhI8bluZ2vR"
      },
      "source": [
        "As we said in the beginning, tech companies and university labs have more computational resources than we do.\n",
        "\n",
        "üî• The [**Visual Geometry Group**](https://www.robots.ox.ac.uk/~vgg/data/) *(Oxford University, Department of Science and Engineering)* became famous for some of their **Very Deep Convolutional Neural Networks**: the [**VGG16**](https://www.robots.ox.ac.uk/~vgg/research/very_deep/)\n",
        "\n",
        "Take 7 minutes of your time to watch this incredible video of Convolutional Layers created by Dimitri Dmitriev.\n",
        "\n",
        "* üì∫ **[VGG16 Neural Network Visualization](https://www.youtube.com/watch?v=RNnKtNrsrmg)**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZldqCInZ2vS"
      },
      "source": [
        "### (5.1) Load VGG16 model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nr6m5eKs9s54"
      },
      "source": [
        "‚ùì **Question: loading the VGG16** ‚ùì\n",
        "\n",
        "* Write a first function `load_model()` that loads the pretrained VGG-16 model from `tensorflow.keras.applications.vgg16`. Have a look at the documentation üìö  [tf/keras/applications/VGG16](https://www.tensorflow.org/api_docs/python/tf/keras/applications/VGG16)üìö\n",
        "\n",
        "* We will **load the VGG16 model** the following way:\n",
        "    - ü§Ø Let's use the **weights** learned on the [**imagenet dataset**](https://www.image-net.org/download.php) (14M pictures with 20k labels)\n",
        "    - The **`input_shape`** corresponds to the input shape of your images\n",
        "        - Note: *You have to resize them down to a consistent shape if they have different height/widths/channels*\n",
        "    - The **`include_top`** argument should be set to `False`:\n",
        "        - to avoid loading the weights of the fully-connected layers of the VGG16\n",
        "        - and also remove the last layer of the VGG16 which was specifically trained on `imagenet`\n",
        "\n",
        "<i><u>Remark:</u></i> Do not change the default value of the other arguments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "evJWHo9DzKNp"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "\n",
        "def load_model():\n",
        "\n",
        "    model = VGG16(\n",
        "    include_top=False,\n",
        "    weights='imagenet',\n",
        "    input_shape=X_train[0].shape)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3psG3JGF7TO"
      },
      "source": [
        "‚ùì **Question: number of parameters in the VGG16** ‚ùì\n",
        "\n",
        "Look at the architecture of the model using ***.summary()***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "JtBpKLegF7TO",
        "tags": [
          "challengify"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b10ac20-a43e-494e-bbfc-f5539d74056d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "58889256/58889256 [==============================] - 4s 0us/step\n"
          ]
        }
      ],
      "source": [
        "model=load_model()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "haCgBw017mNv",
        "outputId": "def8c95b-a66f-4af0-bb0e-281992b28fc4"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 256, 256, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 256, 256, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 128, 128, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 128, 128, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 128, 128, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 64, 64, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 64, 64, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 64, 64, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 64, 64, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 32, 32, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 32, 32, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 16, 16, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 8, 8, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14714688 (56.13 MB)\n",
            "Trainable params: 14714688 (56.13 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1IAwxRVF7TO"
      },
      "source": [
        "<img src=\"https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png\">\n",
        "\n",
        "üí™ Impressive, right? Two things to notice:\n",
        "- It ends with a combo Conv2D/MaxPooling2D\n",
        "- The `layers.Flatten` and the `layers.Dense` are not there yet, we need to add them.\n",
        "- There are more than 14,000,000 parameters, which is a lot...\n",
        "    - We could fine-tune them, i.e. update them as we will update the weights of the dense layers, but it will take a lot of time....\n",
        "    - For this reason, we will inform the model that the layers before the flattening will be set non-trainable.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uccFtSM67lNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehMaUN34Z2vT"
      },
      "source": [
        "‚ùì **Question: deactivating the training of the VGG16 paramters** ‚ùì\n",
        "\n",
        "* Write a first function which:\n",
        "    - takes the previous model as the input\n",
        "    - sets the first layers to be non-trainable, by applying **`model.trainable = False`**\n",
        "    - returns the model.\n",
        "\n",
        "* Then inspect the summary of the model to check that the parameters are no longer trainable, they were set to be **`non-trainable`**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "8E67GWO6zKN3"
      },
      "outputs": [],
      "source": [
        "def set_nontrainable_layers(model):\n",
        "\n",
        "    model.trainable = False\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model= set_nontrainable_layers(model)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-WbjFhQ8FTF",
        "outputId": "716cf07a-9116-4536-c09a-13fa3ebad677"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 256, 256, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 256, 256, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 256, 256, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 128, 128, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 128, 128, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 128, 128, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 64, 64, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 64, 64, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 64, 64, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 64, 64, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 32, 32, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 32, 32, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 32, 32, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 16, 16, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 16, 16, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 8, 8, 512)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14714688 (56.13 MB)\n",
            "Trainable params: 0 (0.00 Byte)\n",
            "Non-trainable params: 14714688 (56.13 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4yedT2VF7TP"
      },
      "source": [
        "‚ùì **Question: chaining the pretrained convolutional layers of VGG16 with our own dense layers** ‚ùì\n",
        "\n",
        "We will write a function that adds flattening and dense layers after the convolutional layers. To do so, we cannot directly use the classic `layers.Sequential()` instantiation.\n",
        "\n",
        "For that reason, we will discover another way here. The idea is that we define each layer (or group of layers) separately. Then, we concatenate them. Have a look at this example:\n",
        "\n",
        "---\n",
        "```python\n",
        "base_model = load_model()\n",
        "base_model = set_nontrainable_layers(base_model)\n",
        "flattening_layer = layers.Flatten()\n",
        "dense_layer = layers.Dense(SOME_NUMBER_1, activation='relu')\n",
        "prediction_layer = layers.Dense(SOME_NUMBER_2, activation='APPROPRIATE_ACTIVATION')\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  base_model,\n",
        "  flattening_layer,\n",
        "  dense_layer,\n",
        "  prediction_layer\n",
        "])\n",
        "\n",
        "```\n",
        "---\n",
        "\n",
        "* The first line loads a group of layers which is the previous VGG-16 model.\n",
        "* Then, we set these layers to be non-trainable.\n",
        "* Eventually, we can instantiate as many layers as we want.\n",
        "* Finally, we use the `Sequential` with the sequence of layers that will correspond to our overall neural network.\n",
        "\n",
        "Replicate the following steps by adding:\n",
        "* a flattening layer\n",
        "* two dense layers (the first with 500 neurons) to the previous VGG-16 model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "Ik-0wwiOzKN4"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "def add_last_layers(model):\n",
        "    '''Take a pre-trained model, set its parameters as non-trainable, and add additional trainable layers on top'''\n",
        "    base_model= set_nontrainable_layers(model)\n",
        "    flattened_layer = layers.Flatten()\n",
        "    dense_layer = layers.Dense(500, activation='relu')\n",
        "    pred_layer = layers.Dense(3, activation='softmax')\n",
        "\n",
        "    model = models.Sequential([base_model, flattened_layer, dense_layer, pred_layer])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-24j0psF7TQ"
      },
      "source": [
        "‚ùì **Question: inspect the parameters of a customized VGG16** ‚ùì\n",
        "\n",
        "* Now look at the layers and the parameters of your model.\n",
        "* Note that there is a distinction, at the end, between the **trainable** and **non-trainable parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uqGuxsJF7TR",
        "tags": [
          "challengify"
        ]
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxXlnPp5F7TR"
      },
      "source": [
        "‚ùì **Question: building a function that creates a full customized VGG16 and compiles it** ‚ùì\n",
        "\n",
        "* Write a function which builds and compiles your model\n",
        "    * We advise using the _adam_ optimizer with `learning_rate=1e-4`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "DrG186vBzKN4"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import optimizers\n",
        "\n",
        "def build_model():\n",
        "    model = load_model()\n",
        "    model = add_last_layers(model)\n",
        "    model.compile(loss= 'categorical_crossentropy', optimizer = 'adam', metrics = 'accuracy')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model= build_model()"
      ],
      "metadata": {
        "id": "luW67Pv5-rlg"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fr1jtqW--3v4",
        "outputId": "58a82fff-ce86-48bb-f727-10282dc0f8a0"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " vgg16 (Functional)          (None, 8, 8, 512)         14714688  \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 32768)             0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 500)               16384500  \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 3)                 1503      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 31100691 (118.64 MB)\n",
            "Trainable params: 16386003 (62.51 MB)\n",
            "Non-trainable params: 14714688 (56.13 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S96qsiKxZ2vU"
      },
      "source": [
        "### (5.2) Back to the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rbkwOw1eF7TS"
      },
      "source": [
        "üö® The VGG16 model was trained on images which were preprocessed in a specific way. This is the reason why we did _NOT_ normalize them earlier.\n",
        "\n",
        "‚ùì **Question: preprocessing the dataset** ‚ùì\n",
        "\n",
        "Apply the specific processing to the original (non-normalized) images here using the method **`preprocess_input`** that you can import from **`tensorflow.keras.applications.vgg16`**\n",
        "\n",
        "üìö Cf. [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/applications/vgg16/preprocess_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "y_mP-xTazKN4"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.vgg16 import preprocess_input"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pojkCqSdAXcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "uNeJZvtV3YDf",
        "tags": [
          "challengify"
        ]
      },
      "outputs": [],
      "source": [
        "X_train = preprocess_input(X_train)\n",
        "X_test = preprocess_input(X_test)\n",
        "X_val = preprocess_input(X_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2T7HvbQfZ2vZ"
      },
      "source": [
        "### (5.3)  Fit the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wu2H0KZF-EoI"
      },
      "source": [
        "\n",
        "\n",
        "‚ùì **Question: Training the customized VGG16** ‚ùì\n",
        "\n",
        "* Train the model with an Early stopping criterion on the validation accuracy -\n",
        "* Since the validation data is provided use `validation_data` instead of `validation_split`.\n",
        "\n",
        "_As usual, store the results of your training into a `history` variable._"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "es = EarlyStopping(patience = 5, restore_best_weights=True)"
      ],
      "metadata": {
        "id": "e2DhVmTdAJNj"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "grmnNmjeAXcQ",
        "tags": [
          "challengify"
        ],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88b50da8-fe61-4a45-cdbb-80adfb0ccba2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "36/36 [==============================] - 14s 256ms/step - loss: 21.7976 - accuracy: 0.7986 - val_loss: 9.2495 - val_accuracy: 0.8547\n",
            "Epoch 2/100\n",
            "36/36 [==============================] - 4s 119ms/step - loss: 2.3568 - accuracy: 0.9282 - val_loss: 9.1993 - val_accuracy: 0.8268\n",
            "Epoch 3/100\n",
            "36/36 [==============================] - 4s 120ms/step - loss: 0.2255 - accuracy: 0.9860 - val_loss: 4.9654 - val_accuracy: 0.8771\n",
            "Epoch 4/100\n",
            "36/36 [==============================] - 4s 123ms/step - loss: 0.1836 - accuracy: 0.9982 - val_loss: 4.8787 - val_accuracy: 0.8994\n",
            "Epoch 5/100\n",
            "36/36 [==============================] - 4s 104ms/step - loss: 0.0059 - accuracy: 0.9965 - val_loss: 5.3113 - val_accuracy: 0.8715\n",
            "Epoch 6/100\n",
            "36/36 [==============================] - 4s 105ms/step - loss: 0.0304 - accuracy: 0.9947 - val_loss: 6.0688 - val_accuracy: 0.8827\n",
            "Epoch 7/100\n",
            "36/36 [==============================] - 4s 107ms/step - loss: 0.0292 - accuracy: 0.9947 - val_loss: 6.7330 - val_accuracy: 0.8827\n",
            "Epoch 8/100\n",
            "36/36 [==============================] - 4s 117ms/step - loss: 0.3786 - accuracy: 0.9807 - val_loss: 17.9912 - val_accuracy: 0.7989\n",
            "Epoch 9/100\n",
            "36/36 [==============================] - 4s 121ms/step - loss: 0.7001 - accuracy: 0.9825 - val_loss: 25.8510 - val_accuracy: 0.7989\n"
          ]
        }
      ],
      "source": [
        "history = model.fit(X_train, y_train, validation_data = (X_val, y_val), epochs = 100, batch_size=16, verbose = 1, callbacks = [es])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgcZFhDTAo8O",
        "outputId": "7047efa8-6a66-45b9-a436-c9026ad872a2"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5/5 [==============================] - 12s 1s/step - loss: 5.9252 - accuracy: 0.8255\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[5.925187110900879, 0.8255033493041992]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec_I9JpiAm-W"
      },
      "source": [
        "‚ùì **Question: Looking at the accuracy** ‚ùì\n",
        "\n",
        "Plot the accuracy for both the train set and and the validation set using the usual function below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1qoEKdWfzKN5"
      },
      "outputs": [],
      "source": [
        "def plot_history(history, title='', axs=None, exp_name=\"\"):\n",
        "    if axs is not None:\n",
        "        ax1, ax2 = axs\n",
        "    else:\n",
        "        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "\n",
        "    if len(exp_name) > 0 and exp_name[0] != '_':\n",
        "        exp_name = '_' + exp_name\n",
        "    ax1.plot(history.history['loss'], label='train' + exp_name)\n",
        "    ax1.plot(history.history['val_loss'], label='val' + exp_name)\n",
        "    #ax1.set_ylim(0., 2.2)\n",
        "    ax1.set_title('loss')\n",
        "    ax1.legend()\n",
        "\n",
        "    ax2.plot(history.history['accuracy'], label='train accuracy'  + exp_name)\n",
        "    ax2.plot(history.history['val_accuracy'], label='val accuracy'  + exp_name)\n",
        "    #ax2.set_ylim(0.25, 1.)\n",
        "    ax2.set_title('Accuracy')\n",
        "    ax2.legend()\n",
        "    return (ax1, ax2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESzinGOY6aBc",
        "tags": [
          "challengify"
        ]
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3plexlQAtcC"
      },
      "source": [
        "‚ùì **Question: Evaluating the model** ‚ùì\n",
        "\n",
        "Evaluate the customized VGG16 accuracy on the test set. Did we improve?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ps_9HwUyRVj9",
        "tags": [
          "challengify"
        ]
      },
      "outputs": [],
      "source": [
        "# YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N5T1KvsGZ2va"
      },
      "source": [
        "## (6) (Optional) Improve the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF39HIb7BSOy"
      },
      "source": [
        "Now, you can try to improve the model's test accuracy. To do that, here are some options you can consider\n",
        "\n",
        "1. **Unfreeze and finetune**: Source: [Google tutorial](https://www.tensorflow.org/guide/keras/transfer_learning#fine-tuning)\n",
        ">_Once your model has converged on the new data, you can try to unfreeze all or part of the base model and retrain the whole model end-to-end with a very low learning rate. This is an optional last step that can potentially give you incremental improvements. It could also potentially lead to quick overfitting -- keep that in mind. It is critical to only do this step after the model with frozen layers has been trained to convergence. If you mix randomly-initialized trainable layers with trainable layers that hold pre-trained features, the randomly-initialized layers will cause very large gradient updates during training, which will destroy your pre-trained features. It's also critical to use a very low learning rate at this stage, because you are training a much larger model than in the first round of training, on a dataset that is typically very small. As a result, you are at risk of overfitting very quickly if you apply large weight updates. Here, you only want to readapt the pretrained weights in an incremental way._\n",
        "\n",
        "\n",
        "1. Add **Data Augmentation** if your model is overfitting.\n",
        "\n",
        "2. If your model is not overfitting, try a more complex model.\n",
        "\n",
        "3. Perform a precise **Grid Search** on all the hyper-parameters: learning_rate, batch_size, data augmentation etc...\n",
        "\n",
        "4. **Change the base model** to more modern one CNN (ResNet, EfficientNet1,... available in the keras library)\n",
        "\n",
        "5. Curate the data: maintaining a sane data set is one of the keys to success.\n",
        "\n",
        "6. Collect more data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3UMNBZHZ2vb"
      },
      "source": [
        "## (6.2) Comparing the performances of the CNN, the VGG, and the VGG trained on the augmented dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NIRxxujzKN5"
      },
      "outputs": [],
      "source": [
        "test_accuracy_aug = res_aug[-1]\n",
        "\n",
        "\n",
        "print(f\"test_accuracy_aug = {round(test_accuracy_aug,2)*100} %\")\n",
        "\n",
        "print(f\"test_accuracy_vgg = {round(test_accuracy_vgg,2)*100} %\")\n",
        "\n",
        "print(f\"test_accuracy = {round(test_accuracy,2)*100} %\")\n",
        "\n",
        "print(f'Chance level: {1./num_classes*100:.1f}%')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8gaSAxLZ2vc"
      },
      "source": [
        "---\n",
        "\n",
        "üèÅ **Congratulations** üèÅ\n",
        "\n",
        "1. Download this notebook from your `Google Drive` or directly from `Google Colab`\n",
        "2. Drag-and-drop it from your `Downloads` folder to your local challenge folder  \n",
        "\n",
        "\n",
        "üíæ Don't forget to push your code\n",
        "\n",
        "3. Follow the usual procedure on your terminal inside the challenge folder:\n",
        "      * *git add transfer_learning.ipynb*\n",
        "      * *git commit -m \"I am the god of Transfer Learning\"*\n",
        "      * *git push origin master*\n",
        "\n",
        "*Hint*: To find where this Colab notebook has been saved, click on `File` $\\rightarrow$ `Locate in Drive`.\n",
        "\n",
        "üöÄ If you have time, move on to the **Autoencoders** challenge!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcbJhV0xzKN6"
      },
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}